# MLOps Stack Canvas
## Chest X-ray Multi-Disease Classification with DenseNet121 (Deep Learning)

---

### **PROJECT NAME:**
Chest X-ray Multi-Disease Classification with DenseNet121

### **DATE:**
2025-11-21

### **TEAM MEMBERS:**
- Name 1: [à¹ƒà¸ªà¹ˆà¸Šà¸·à¹ˆà¸­à¸ªà¸¡à¸²à¸Šà¸´à¸ 1]
- Name 2: [à¹ƒà¸ªà¹ˆà¸Šà¸·à¹ˆà¸­à¸ªà¸¡à¸²à¸Šà¸´à¸ 2]
- Name 3: [à¹ƒà¸ªà¹ˆà¸Šà¸·à¹ˆà¸­à¸ªà¸¡à¸²à¸Šà¸´à¸ 3]
- Name 4: [à¹ƒà¸ªà¹ˆà¸Šà¸·à¹ˆà¸­à¸ªà¸¡à¸²à¸Šà¸´à¸ 4]

---

## ğŸ¯ 1. VALUE PROPOSITION
### *Keep solving the real problem*

#### **ğŸ¥ Primary Value**
- **Multi-Disease Detection**: à¸•à¸£à¸§à¸ˆà¸ˆà¸±à¸šà¹‚à¸£à¸„à¹ƒà¸™à¸ à¸²à¸ X-ray à¸ˆà¸³à¸™à¸§à¸™ **15 à¸Šà¸™à¸´à¸”** à¹€à¸Šà¹ˆà¸™:
    - Atelectasis (à¸›à¸­à¸”à¸¢à¸¸à¸š)
    - Cardiomegaly (à¸«à¸±à¸§à¹ƒà¸ˆà¹‚à¸•)
    - Consolidation (à¸à¸²à¸£à¹à¸‚à¹‡à¸‡à¸•à¸±à¸§à¸‚à¸­à¸‡à¸›à¸­à¸”)
    - Edema (à¸šà¸§à¸¡à¸™à¹‰à¸³)
    - Effusion (à¸™à¹‰à¸³à¹ƒà¸™à¹€à¸¢à¸·à¹ˆà¸­à¸«à¸¸à¹‰à¸¡à¸›à¸­à¸”)
    - Emphysema (à¸–à¸¸à¸‡à¸¥à¸¡à¹‚à¸›à¹ˆà¸‡à¸à¸­à¸‡)
    - Fibrosis (à¹€à¸ªà¹‰à¸™à¹ƒà¸¢à¹ƒà¸™à¸›à¸­à¸”)
    - Hernia (à¹€à¸®à¸­à¸£à¹Œà¹€à¸™à¸µà¸¢)
    - Infiltration (à¸à¸²à¸£à¹à¸—à¸£à¸à¸‹à¸¶à¸¡)
    - Mass (à¸à¹‰à¸­à¸™)
    - No Finding (à¸›à¸à¸•à¸´)
    - Nodule (à¸›à¸¡)
    - Pleural Thickening (à¹€à¸¢à¸·à¹ˆà¸­à¸«à¸¸à¹‰à¸¡à¸›à¸­à¸”à¸«à¸™à¸²)
    - Pneumonia (à¸›à¸­à¸”à¸­à¸±à¸à¹€à¸ªà¸š)
    - Pneumothorax (à¸¥à¸¡à¸£à¸±à¹ˆà¸§à¹ƒà¸™à¸Šà¹ˆà¸­à¸‡à¸­à¸)

#### **âš¡ Clinical Benefits**
- **Time Efficiency**: à¸Šà¹ˆà¸§à¸¢à¹à¸à¸—à¸¢à¹Œà¸¥à¸”à¹€à¸§à¸¥à¸²à¹ƒà¸™à¸à¸²à¸£à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸ à¸²à¸ X-ray
- **Error Reduction**: à¸¥à¸”à¸„à¸§à¸²à¸¡à¸œà¸´à¸”à¸à¸¥à¸²à¸”à¸ˆà¸²à¸ human fatigue
- **24/7 Availability**: à¸£à¸­à¸‡à¸£à¸±à¸šà¸à¸²à¸£à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¸•à¹ˆà¸­à¹€à¸™à¸·à¹ˆà¸­à¸‡

#### **ğŸ” Explainability**
- **Grad-CAM Integration**: à¸¡à¸µ Grad-CAM à¹€à¸à¸·à¹ˆà¸­à¸Šà¹ˆà¸§à¸¢à¸­à¸˜à¸´à¸šà¸²à¸¢à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ (Explainability)
- **Clinical Trust**: à¹€à¸«à¸¡à¸²à¸°à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸²à¸£à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¸ˆà¸£à¸´à¸‡à¸—à¸²à¸‡à¸„à¸¥à¸´à¸™à¸´à¸
- **PACS Integration**: à¸£à¸­à¸‡à¸£à¸±à¸šà¸à¸²à¸£à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¹ƒà¸™à¹‚à¸£à¸‡à¸à¸¢à¸²à¸šà¸²à¸¥à¸«à¸£à¸·à¸­à¸£à¸°à¸šà¸š PACS

---

## ğŸ“Š 2. DATA SOURCES & DATA VERSIONING

#### **ğŸ“ Primary Dataset**
```
Dataset: NIH Chest X-ray Dataset
â”œâ”€â”€ Total Images: 112,120 images
â”œâ”€â”€ Format: .jpg files
â”œâ”€â”€ Labels: .csv metadata
â””â”€â”€ Classes: 15 disease categories
```

#### **ğŸ—‚ï¸ Data Organization**
- **Source**: NIH Chest X-ray Dataset (ChestX-ray14)
- **Format**: High-resolution JPEG images + CSV labels
- **Size**: ~42GB total dataset

#### **ğŸ“ˆ Data Versioning Strategy**
```
Data Pipeline:
Raw Data â†’ Cleaned Data â†’ Augmented Data â†’ Training Ready
    â†“           â†“             â†“              â†“
   v1.0       v1.1          v1.2           v1.3
```

**Tools & Methods:**
- **Git LFS**: à¸ªà¸³à¸«à¸£à¸±à¸š large file versioning
- **DVC (Data Version Control)**: à¸ªà¸³à¸«à¸£à¸±à¸š dataset versioning
- **Metadata Tracking**: à¸šà¸±à¸™à¸—à¸¶à¸ resolution, preprocessing steps
- **Data Lineage**: track data transformation history

#### **ğŸ“‹ Data Quality Assurance**
- Image integrity validation
- Resolution consistency check
- Label quality verification
- Missing data handling

**Implementation:**
- à¹ƒà¸Šà¹‰ `src/check_images.py` à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸„à¸§à¸²à¸¡à¸ªà¸¡à¸šà¸¹à¸£à¸“à¹Œà¸‚à¸­à¸‡à¸£à¸¹à¸›à¸ à¸²à¸ (missing, corrupted, invalid extension)
- à¸ªà¸£à¹‰à¸²à¸‡ clean CSV à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´ (archive/clean_labels.csv)
- à¸£à¸²à¸¢à¸‡à¸²à¸™à¸›à¸±à¸à¸«à¸²à¹ƒà¸™ image_validation_report.txt

---

## ğŸ”¬ 3. DATA ANALYSIS & EXPERIMENT MANAGEMENT

#### **ğŸ“Š Exploratory Data Analysis**
```python
# Analysis Tools
â”œâ”€â”€ Jupyter Notebook
â”œâ”€â”€ pandas (data manipulation)
â”œâ”€â”€ matplotlib + seaborn (visualization)
â”œâ”€â”€ numpy (numerical operations)
â””â”€â”€ PIL (image processing)
```

#### **ğŸ¯ Key Analysis Areas**
- **Class Distribution**: à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œ class imbalance à¸‚à¸­à¸‡à¹‚à¸£à¸„à¹à¸•à¹ˆà¸¥à¸°à¸Šà¸™à¸´à¸”
- **Image Quality**: à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š blur, noise, artifacts
- **Resolution Analysis**: à¸„à¸§à¸²à¸¡à¸«à¸¥à¸²à¸à¸«à¸¥à¸²à¸¢à¸‚à¸­à¸‡à¸‚à¸™à¸²à¸”à¸ à¸²à¸
- **Medical Annotations**: à¸„à¸§à¸²à¸¡à¸–à¸¹à¸à¸•à¹‰à¸­à¸‡à¸‚à¸­à¸‡ ground truth

#### **ğŸ§ª Experiment Tracking**
**Primary Tools:**
- **Weights & Biases (wandb)**: à¸ªà¸³à¸«à¸£à¸±à¸š comprehensive experiment tracking
- **TensorBoard**: à¸ªà¸³à¸«à¸£à¸±à¸š real-time monitoring
- **MLflow**: à¸ªà¸³à¸«à¸£à¸±à¸š model lifecycle management

**Implementation:**
- Mixed Precision Training (TensorFlow)
- Automatic Model Checkpointing (best_weights.weights.h5)
- Real-time Training Metrics (F1-score, AUC, Loss)
- Grad-CAM Visualization integrated in prediction

**Tracked Parameters:**
```yaml
Hyperparameters:
  - learning_rate: [1e-4, 5e-4, 1e-3]
  - optimizer: [Adam, AdamW, SGD]
  - batch_size: [16, 32, 64]
  - augmentation_strategy: [basic, advanced, custom]

Model Variants:
  - DenseNet121, DenseNet169
  - ResNet50, ResNet101
  - EfficientNet-B0 to B7

Metrics Tracked:
  - Overall Accuracy
  - Per-class F1-score
  - AUC-ROC per disease
  - Training/Validation Loss
  - Inference Time
```

---

## ğŸ­ 4. FEATURE STORE & WORKFLOWS
### *Keep feature computation consistent along the ML lifecycle*

#### **ğŸ”„ Preprocessing Pipeline**
```python
# Shared Feature Workflow
preprocessing_pipeline = {
    'resize': (224, 224),
    'normalize': ImageNet_stats,
    'augmentation': {
        'RandomRotation': 15,
        'RandomHorizontalFlip': 0.5,
        'ColorJitter': (0.1, 0.1, 0.1, 0.1),
        'RandomAffine': 10
    }
}
```

#### **ğŸ› ï¸ Consistent Feature Engineering**
- **Torchvision Transforms**: standardized preprocessing
- **Reproducible Parameters**: à¸—à¸¸à¸ environment à¹ƒà¸Šà¹‰ config à¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸™
- **Version Control**: preprocessing steps à¸¡à¸µ versioning
- **Testing**: unit tests à¸ªà¸³à¸«à¸£à¸±à¸š feature pipeline

#### **ğŸ“¦ Feature Store Implementation**
- **Configuration Management**: YAML/JSON configs
- **Pipeline Orchestration**: reproducible workflows
- **Quality Gates**: automated feature validation

---

## ğŸ—ï¸ 5. FOUNDATIONS
### *Reflecting DevOps*

#### **ğŸ“ Version Control Strategy**
```
Repository Structure:
â”œâ”€â”€ src/               # Source code
â”œâ”€â”€ configs/           # Configuration files
â”œâ”€â”€ tests/             # Unit tests
â”œâ”€â”€ docker/            # Docker configurations
â”œâ”€â”€ .github/workflows/ # CI/CD pipelines
â””â”€â”€ docs/              # Documentation
```

**Branching Strategy:**
```
main (production)
â”œâ”€â”€ staging (pre-production)
â”œâ”€â”€ develop (integration)
â””â”€â”€ feature/* (development)
```

#### **âš™ï¸ Development Environment**
- **GitHub/GitLab**: primary version control
- **Code Quality**: 
  - **flake8**: linting
  - **black**: code formatting
  - **mypy**: type checking
- **Testing Framework**: **pytest** for unit tests
- **Pre-commit Hooks**: automated quality checks

**Troubleshooting & Resource Management:**
- à¸„à¸¹à¹ˆà¸¡à¸·à¸­à¹à¸à¹‰à¸›à¸±à¸à¸«à¸² GPU, memory, module error (à¸”à¸¹ README.md section troubleshooting)
- à¸›à¸£à¸±à¸š batch size, learning rate, epochs à¹„à¸”à¹‰à¹ƒà¸™ train.py

#### **ğŸ–¥ï¸ Infrastructure Requirements**
```
Hardware Specifications:
â”œâ”€â”€ Training: GPU-enabled (RTX 3080+ or V100+)
â”œâ”€â”€ Inference: CPU/GPU hybrid
â”œâ”€â”€ Storage: SSD for fast I/O
â””â”€â”€ Memory: 32GB+ RAM recommended

Deployment Options:
â”œâ”€â”€ Local Development: Docker containers
â”œâ”€â”€ Cloud Training: Google Colab Pro, AWS EC2
â””â”€â”€ Production: Kubernetes clusters
```

---

## ğŸ”„ 6. CI/CT/CD: ML PIPELINE ORCHESTRATION
### *Building, testing, packaging and deploying ML pipeline*

#### **ğŸ”¨ Continuous Integration (CI)**
```yaml
# GitHub Actions Workflow
CI Pipeline:
  - Code Quality Check:
    â”œâ”€â”€ flake8 (linting)
    â”œâ”€â”€ black (formatting)
    â”œâ”€â”€ mypy (type checking)
    â””â”€â”€ pytest (unit tests)
  
  - Data Validation:
    â”œâ”€â”€ Schema validation
    â”œâ”€â”€ Data integrity checks
    â””â”€â”€ Feature pipeline tests
  
  - Model Testing:
    â”œâ”€â”€ Model architecture tests
    â”œâ”€â”€ Training pipeline validation
    â””â”€â”€ Inference functionality
```

#### **ğŸ¯ Continuous Training (CT)**
```python
# Automated Retraining Triggers
retraining_conditions = {
    'data_drift': threshold_0.1,
    'performance_degradation': accuracy_drop_5_percent,
    'new_data_volume': min_1000_samples,
    'scheduled': weekly_retraining
}
```

#### **ğŸš€ Continuous Deployment (CD)**
```
Deployment Pipeline:
Raw Model â†’ Model Validation â†’ Artifact Creation â†’ Registry Push â†’ Deployment

Tools Stack:
â”œâ”€â”€ GitHub Actions: automation
â”œâ”€â”€ DVC Pipelines: ML workflow orchestration
â”œâ”€â”€ Docker: containerization
â”œâ”€â”€ FastAPI: model serving
â””â”€â”€ Kubernetes: scaling (production)
```

---

## ğŸ“¦ 7. MODEL REGISTRY & MODEL VERSIONING

#### **ğŸ›ï¸ Model Registry Strategy**
```
MLflow Model Registry Structure:
â”œâ”€â”€ Production Models:
â”‚   â”œâ”€â”€ best_accuracy_v2.1.pt
â”‚   â”œâ”€â”€ best_auc_v2.0.pt
â”‚   â””â”€â”€ current_production.pt
â”œâ”€â”€ Staging Models:
â”‚   â”œâ”€â”€ candidate_v2.2.pt
â”‚   â””â”€â”€ experimental_v3.0.pt
â””â”€â”€ Archive:
    â”œâ”€â”€ baseline_v1.0.pt
    â””â”€â”€ lightweight_edge_v1.5.pt
```

#### **ğŸ“Š Model Metadata Tracking**
```json
{
  "model_version": "v2.1",
  "training_config": {
    "architecture": "DenseNet121",
    "learning_rate": 1e-4,
    "batch_size": 32,
    "epochs": 50
  },
  "dataset_version": "v1.3",
  "performance_metrics": {
    "overall_accuracy": 0.847,
    "mean_auc": 0.912,
    "per_class_f1": {...}
  },
  "training_time": "2.5 hours",
  "model_size": "28.7 MB"
}
```

#### **ğŸ”„ Model Lifecycle Management**
- **Automatic Promotion**: staging â†’ production based on metrics
- **Rollback Strategy**: instant fallback to previous stable version
- **A/B Testing**: gradual rollout of new models
- **Model Retirement**: systematic deprecation process

**Artifact Storage:**
- Model weights: `saved_models/best_weights.weights.h5`
- Training logs & metrics: `saved_models/` à¹à¸¥à¸° log files

---

## ğŸš€ 8. MODEL DEPLOYMENT

#### **ğŸŒ Deployment Architecture**
```
Production Deployment Stack:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Load        â”‚    â”‚ API Gateway  â”‚    â”‚ Model       â”‚
â”‚ Balancer    â”‚â”€â”€â”€â–¶â”‚ (FastAPI)    â”‚â”€â”€â”€â–¶â”‚ Service     â”‚
â”‚ (Nginx)     â”‚    â”‚              â”‚    â”‚ (GPU/CPU)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                    â”‚                   â”‚
       â–¼                    â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Monitoring  â”‚    â”‚ Logging      â”‚    â”‚ Model       â”‚
â”‚ (Prometheus)â”‚    â”‚ (ELK Stack)  â”‚    â”‚ Storage     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### **ğŸ”§ Deployment Options**

**Option 1: REST API (Recommended)**
```python
# FastAPI Implementation
@app.post("/predict")
async def predict_disease(image: UploadFile):
    # Process image
    # Run inference
    # Return results + confidence scores

@app.post("/gradcam")
async def generate_gradcam(image: UploadFile, target_class: str):
    # Generate interpretability visualization
    # Return heatmap overlay
```

**Option 2: Containerized Deployment**
```dockerfile
# Multi-stage Docker build
FROM pytorch/pytorch:1.12-cuda11.3-cudnn8-runtime
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY src/ ./src/
EXPOSE 8000
CMD ["uvicorn", "src.app:app", "--host", "0.0.0.0", "--port", "8000"]
```

**Option 3: Cloud Deployment**
- **AWS**: ECS/EKS with GPU support
- **GCP**: Cloud Run with custom containers
- **Azure**: Container Instances with GPU

#### **âš¡ Scaling Strategy**
- **Horizontal Scaling**: Kubernetes auto-scaling
- **Load Balancing**: distribute requests across replicas
- **GPU Optimization**: efficient GPU memory management
- **Caching**: Redis for frequent predictions

---

## ğŸ¯ 9. PREDICTION SERVING

#### **ğŸ”Œ API Endpoints**
```python
# Primary Inference Endpoint
POST /api/v1/predict
{
    "image": "base64_encoded_xray_image",
    "return_probabilities": true,
    "threshold": 0.5
}

Response:
{
    "predictions": {
        "Pneumonia": 0.85,
        "Mass": 0.12,
        "Nodule": 0.08,
        ...
    },
    "processing_time": 150,  # milliseconds
    "model_version": "v2.1"
}

# Interpretability Endpoint
POST /api/v1/gradcam
{
    "image": "base64_encoded_xray_image",
    "target_classes": ["Pneumonia", "Mass"]
}

Response:
{
    "gradcam_images": {
        "Pneumonia": "base64_encoded_heatmap",
        "Mass": "base64_encoded_heatmap"
    },
    "processing_time": 300
}
```

#### **âš¡ Performance Requirements**
- **Response Time**: < 200ms (with GPU)
- **Throughput**: 100+ requests/minute
- **Availability**: 99.9% uptime
- **Scalability**: auto-scale based on load

#### **ğŸ›¡ï¸ API Features**
- **Authentication**: API key management
- **Rate Limiting**: prevent abuse
- **Input Validation**: image format/size checks
- **Error Handling**: comprehensive error responses
- **Monitoring**: request/response logging

---

## ğŸ“Š 10. MODEL & DATA & APPLICATION MONITORING

#### **ğŸ¯ Performance Monitoring**
```python
# Key Metrics Dashboard
monitoring_metrics = {
    'Model Performance': {
        'accuracy_drift': 'weekly_comparison',
        'prediction_confidence': 'distribution_tracking',
        'class_distribution': 'input_data_monitoring'
    },
    'System Performance': {
        'response_time': 'p95_latency',
        'throughput': 'requests_per_minute',
        'error_rate': 'percentage_failed_requests'
    },
    'Infrastructure': {
        'gpu_utilization': 'percentage',
        'memory_usage': 'gb_consumed',
        'disk_space': 'storage_available'
    }
}
```

#### **ğŸš¨ Drift Detection**
**Data Drift Monitoring:**
- **Image Quality**: brightness, contrast distribution changes
- **Input Distribution**: new patient demographics
- **Technical Drift**: different X-ray machines/protocols

**Model Drift Detection:**
- **Performance Degradation**: accuracy drop > 5%
- **Confidence Score Changes**: unusual prediction patterns
- **Class Imbalance Shifts**: changing disease prevalence

#### **ğŸ› ï¸ Monitoring Tools Stack**
```
Monitoring Infrastructure:
â”œâ”€â”€ Prometheus: metrics collection
â”œâ”€â”€ Grafana: visualization dashboards
â”œâ”€â”€ MLflow: ML-specific monitoring
â”œâ”€â”€ ELK Stack: log analysis
â””â”€â”€ AlertManager: notification system

Alert Conditions:
â”œâ”€â”€ Accuracy drop > 5%
â”œâ”€â”€ Response time > 500ms
â”œâ”€â”€ Error rate > 1%
â””â”€â”€ GPU utilization > 90%
```

#### **ğŸ“Š Real-time Dashboards**
- **Clinical Dashboard**: per-disease accuracy trends
- **Technical Dashboard**: system performance metrics
- **Business Dashboard**: usage statistics and ROI

---

## âš ï¸ 0. MLOPS DILEMMAS

#### **ğŸ¯ Technical Challenges**

**Dataset Imbalance Crisis:**
```
Disease Distribution Problem:
â”œâ”€â”€ Common: Infiltration (19%), No Finding (60%)
â”œâ”€â”€ Rare: Pneumothorax (5%), Hernia (0.2%)
â””â”€â”€ Impact: Low recall for rare diseases

Solutions Implemented:
â”œâ”€â”€ Weighted Loss Functions
â”œâ”€â”€ Focal Loss for hard examples
â”œâ”€â”€ SMOTE-like augmentation
â””â”€â”€ Ensemble methods
```

**Image Quality Variability:**
- **Resolution Inconsistency**: 1024x1024 to 4892x4020 pixels
- **Equipment Variations**: different X-ray machines
- **Preprocessing Robustness**: need adaptive normalization

**Interpretability Reliability:**
- **Grad-CAM Limitations**: sometimes highlights irrelevant regions
- **Clinical Validation**: need radiologist confirmation
- **False Confidence**: high predictions on uncertain cases

#### **ğŸ”’ Data Privacy & Compliance**
```
HIPAA Compliance Requirements:
â”œâ”€â”€ Data Anonymization: remove all PHI
â”œâ”€â”€ Secure Storage: encrypted data at rest
â”œâ”€â”€ Access Control: role-based permissions
â”œâ”€â”€ Audit Logging: track all data access
â””â”€â”€ Data Retention: automatic deletion policies
```

#### **ğŸ’° Resource Optimization**
**GPU Cost Management:**
- **Training Costs**: $50-200 per full training run
- **Inference Optimization**: batch processing for efficiency
- **Auto-scaling**: cost-effective resource allocation
- **Model Compression**: reduce computational requirements

#### **ğŸ”„ Operational Challenges**
- **Model Staleness**: when to retrain?
- **Version Management**: seamless model updates
- **Downtime Management**: zero-downtime deployments
- **Performance Degradation**: early warning systems

---

## ğŸ—„ï¸ 11. METADATA STORE

#### **ğŸ“Š Comprehensive Metadata Management**
```json
{
  "metadata_categories": {
    "data_versions": {
      "dataset_v1.3": {
        "source": "NIH_ChestX-ray14",
        "size": "112,120_images",
        "preprocessing": "resize_224x224_normalize_imagenet",
        "splits": "train_70_val_15_test_15",
        "creation_date": "2025-11-15"
      }
    },
    "model_versions": {
      "densenet121_v2.1": {
        "architecture": "DenseNet121_pretrained",
        "fine_tuning": "full_model",
        "hyperparameters": {
          "learning_rate": 1e-4,
          "batch_size": 32,
          "optimizer": "AdamW",
          "scheduler": "CosineAnnealingLR"
        },
        "training_duration": "2.5_hours",
        "gpu_used": "NVIDIA_RTX_3080"
      }
    },
    "experiment_configs": {
      "exp_042": {
        "objective": "improve_rare_disease_recall",
        "modifications": "focal_loss_alpha_0.75",
        "results": "improved_pneumothorax_recall_15_percent"
      }
    },
    "evaluation_results": {
      "per_class_metrics": {
        "Pneumonia": {"precision": 0.89, "recall": 0.85, "f1": 0.87},
        "Mass": {"precision": 0.78, "recall": 0.72, "f1": 0.75},
        "Nodule": {"precision": 0.82, "recall": 0.79, "f1": 0.80}
      },
      "overall_metrics": {
        "macro_avg_f1": 0.81,
        "weighted_avg_f1": 0.84,
        "mean_auc_roc": 0.91
      }
    }
  }
}
```

#### **ğŸ› ï¸ Metadata Tools & Storage**
```
Metadata Infrastructure:
â”œâ”€â”€ MLflow Tracking: experiment metadata
â”œâ”€â”€ DVC: data and model versioning metadata
â”œâ”€â”€ PostgreSQL: structured metadata storage
â”œâ”€â”€ MongoDB: unstructured experiment notes
â””â”€â”€ Git: code and config versioning

**Contribution Workflow:**
- Fork, Branch, Pull Request à¸•à¸²à¸¡à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¹ƒà¸™ README.md
```

---

## ğŸ“ Project Structure (implementation)
```
train-LLM-Chest-X-rays/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ dataset.py          # Data handling
â”‚   â”œâ”€â”€ model.py           # DenseNet121 model
â”‚   â”œâ”€â”€ train.py           # Training pipeline
â”‚   â”œâ”€â”€ predict.py         # Prediction & Grad-CAM
â”‚   â”œâ”€â”€ check_images.py    # Image validation tool
â”‚   â””â”€â”€ utils.py           # Utilities
â”œâ”€â”€ archive/               # Training data
â”‚   â”œâ”€â”€ new_labels.csv     # Labels
â”‚   â””â”€â”€ resized_images/    # Images
â”œâ”€â”€ saved_models/          # Trained model weights
â”œâ”€â”€ predict_images/        # Images for prediction
â””â”€â”€ requirements.txt       # Dependencies
```

---

### ğŸ“‹ **Quick Reference Summary**

| **Component** | **Tool/Technology** | **Purpose** |
|--------------|-------------------|------------|
| **Data Versioning** | DVC + Git LFS | Dataset version control |
| **Experiment Tracking** | Weights & Biases | Hyperparameter tuning |
| **Model Registry** | MLflow | Model lifecycle management |
| **Deployment** | FastAPI + Docker | Model serving |
| **Monitoring** | Prometheus + Grafana | Performance tracking |
| **CI/CD** | GitHub Actions | Automation pipeline |

---

*This MLOps Stack Canvas provides a comprehensive blueprint for implementing a production-ready chest X-ray classification system with proper MLOps practices and governance.*